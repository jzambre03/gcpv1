# Risk Assessment & Confidence Scoring System
## Complete Technical Documentation

**Version:** 1.0  
**Last Updated:** December 16, 2025  
**System:** Golden Config AI - Multi-Agent System

---

## üìã Table of Contents

1. [System Overview](#system-overview)
2. [Risk Assessment for Individual Deltas](#risk-assessment-for-individual-deltas)
3. [Confidence Score Calculation](#confidence-score-calculation)
4. [Intelligent Scoring Rules](#intelligent-scoring-rules)
5. [Decision Thresholds](#decision-thresholds)
6. [Real-World Examples](#real-world-examples)
7. [Code Walkthrough](#code-walkthrough)
8. [LLM Integration](#llm-integration)
9. [Testing & Verification](#testing--verification)

---

## 1. System Overview

### Architecture

The Risk Assessment & Confidence Scoring System is a **hybrid intelligent system** that combines:

1. **DETERMINISTIC BRAIN (Rules Engine)**
   - Pattern matching for keywords (password, secret, token)
   - Category-based risk classification
   - Hard-coded safety thresholds
   - Policy violation detection

2. **LLM BRAIN (Contextual Intelligence)**
   - Semantic understanding of configuration changes
   - Contextual risk analysis based on file type, environment, and business logic
   - Pattern recognition for anomalies (e.g., typos, unknown service IDs)
   - Historical learning from past incidents
   - Blast radius analysis

3. **SCORING BRAIN (Confidence Calculation)**
   - Combines deterministic and LLM outputs
   - Applies weighted deductions and bonuses
   - Environment-aware thresholds
   - Final certification decision (AUTO_MERGE, HUMAN_REVIEW, BLOCK_MERGE)

### Processing Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Configuration Drift Detection                               ‚îÇ
‚îÇ    (Drift Detector Agent compares Golden vs Candidate)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. DETERMINISTIC RISK ASSESSMENT                                ‚îÇ
‚îÇ    (Rule-based categorization in drift_v1.py)                   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ    Input: Raw delta (file, locator, old, new)                  ‚îÇ
‚îÇ    Process: _risk_level_and_reason()                            ‚îÇ
‚îÇ    Output: risk_level (high, med, low) + risk_reason           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. POLICY TAGGING                                               ‚îÇ
‚îÇ    (Policy Engine applies policy rules)                         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ    Input: Delta + Policy rules (YAML)                          ‚îÇ
‚îÇ    Process: _tag_with_policy()                                  ‚îÇ
‚îÇ    Output: policy_tag (suspect, allowed_variance, breach)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. LLM CATEGORIZATION                                           ‚îÇ
‚îÇ    (Triaging/Routing Agent - Claude LLM)                        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ    Input: Deltas + Environment + Policy context                ‚îÇ
‚îÇ    Process: build_llm_format_prompt() ‚Üí Claude API             ‚îÇ
‚îÇ    Output: {high: [], medium: [], low: [], allowed_variance: []}‚îÇ
‚îÇ           + ai_review_assistant (risk analysis, actions)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. CONFIDENCE SCORE CALCULATION                                 ‚îÇ
‚îÇ    (Certification Engine - ConfidenceScorer)                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ    Input: LLM output + policy violations + risk counts         ‚îÇ
‚îÇ    Process: calculate() method with 7 components               ‚îÇ
‚îÇ    Output: Score (0-100) + Decision + Explanation              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 6. CERTIFICATION DECISION                                       ‚îÇ
‚îÇ    (AUTO_MERGE / HUMAN_REVIEW / BLOCK_MERGE)                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ    Score >= 85 (prod) ‚Üí AUTO_MERGE ‚úÖ                           ‚îÇ
‚îÇ    Score 60-84 (prod) ‚Üí HUMAN_REVIEW ‚ö†Ô∏è                        ‚îÇ
‚îÇ    Score < 60 (prod) ‚Üí BLOCK_MERGE üö´                          ‚îÇ
‚îÇ    + CRITICAL RULE: ANY medium/high/critical ‚Üí BLOCK_MERGE     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2. Risk Assessment for Individual Deltas

### What is a Delta?

A **delta** is a single configuration change detected between two branches (Golden vs Drift/Candidate).

**Example Delta Structure:**
```json
{
  "id": "cfg~application.yml.server.port",
  "category": "config",
  "file": "application.yml",
  "locator": {
    "type": "keypath",
    "value": "application.yml.server.port"
  },
  "old": "8080",
  "new": "8090",
  "risk_level": "med",
  "risk_reason": "Behavioral or version/configuration change.",
  "policy": {
    "tag": "suspect",
    "rule": ""
  }
}
```

### Stage 1: Deterministic Risk Assessment (Rules Brain)

**Location:** `shared/drift_analyzer/drift_v1.py`  
**Function:** `_risk_level_and_reason(d: Dict[str, Any]) -> Tuple[str, str]`

This is the **FIRST LINE OF DEFENSE** - a fast, deterministic rule engine that assigns baseline risk levels based on keywords and categories.

#### Rule Logic (Line by Line)

```python
def _risk_level_and_reason(d: Dict[str, Any]) -> Tuple[str, str]:
    """
    Return (risk_level, risk_reason) where risk_level ‚àà {high, med, low}.
    
    This function runs BEFORE LLM analysis to provide a baseline risk assessment.
    """
    # Extract key fields from delta
    cat = d.get("category","")           # e.g., "config", "dependency", "jenkins"
    file = (d.get("file") or "").lower() # e.g., "application.yml", "pom.xml"
    loc = ((d.get("locator") or {}).get("value") or "").lower()  # e.g., "db.password"
```

**Line 546-548:** Extract the category, file name, and locator value from the delta. All strings are lowercased for case-insensitive matching.

---

#### HIGH RISK Rules

```python
    # HIGH RISK RULE 1: Credential/Secret Keywords
    if any(tok in loc for tok in ("password","secret","token","credentialsid",
                                   "db.password","db.username","jdbc.url","posdb_")):
        return "high", "Sensitive credential or connection parameter changed."
```

**Lines 551-552:**  
- **What it checks:** If the locator contains ANY of these security-sensitive keywords
- **Keywords:** `password`, `secret`, `token`, `credentialsid`, `db.password`, `db.username`, `jdbc.url`, `posdb_`
- **Why HIGH:** These are credentials that can expose databases, APIs, or authentication systems. A leaked or incorrect credential can cause:
  - Security breaches
  - Service outages (if connection fails)
  - Data exposure
- **Example Matches:**
  - `application.yml.spring.datasource.password` ‚Üí HIGH
  - `config.properties.api.secret.token` ‚Üí HIGH
  - `jdbc.url` in any file ‚Üí HIGH

---

```python
    # HIGH RISK RULE 2: Pipeline Credentials & Container Base Images
    if cat in ("jenkins","container") and ("credentials" in loc or "from[" in loc):
        return "high", "Pipeline credential or container base image changed."
```

**Lines 553-554:**  
- **What it checks:** If category is `jenkins` or `container` AND locator contains `credentials` or `from[`
- **Why HIGH:**
  - **Jenkins credentials:** CI/CD pipeline credentials control deployment access
  - **Container base image (from[]):** Changing the base image (e.g., `FROM ubuntu:20.04`) can introduce vulnerabilities, break dependencies, or change runtime behavior
- **Example Matches:**
  - `Jenkinsfile` with `credentialsId` change ‚Üí HIGH
  - `Dockerfile` with `FROM node:14` changed to `FROM node:18` ‚Üí HIGH

---

```python
    # HIGH RISK RULE 3: Production Profile Changes
    if cat == "spring_profile" and ("prod" in file or ".production" in file):
        return "high", "Production profile configuration changed."
```

**Lines 555-556:**  
- **What it checks:** If category is `spring_profile` AND file name contains `prod` or `.production`
- **Why HIGH:** Production-specific Spring profiles control critical runtime behavior. A misconfiguration can cause:
  - Production outages
  - Database connection failures
  - Security misconfigurations (e.g., disabling auth in prod)
- **Example Matches:**
  - `application-prod.yml` ‚Üí HIGH
  - `application.production.properties` ‚Üí HIGH
  - `application-dev.yml` ‚Üí Not matched (not prod)

---

#### MEDIUM RISK Rules

```python
    # MEDIUM RISK: Behavioral Changes
    if cat in ("code_hunk","dependency","build_config","spring_profile","config"):
        return "med", "Behavioral or version/configuration change."
```

**Lines 559-560:**  
- **What it checks:** If category is any of: `code_hunk`, `dependency`, `build_config`, `spring_profile`, `config`
- **Why MEDIUM:** These changes modify application behavior but are not as critical as credentials. They require review but may be intentional.
- **Categories Explained:**
  - `code_hunk`: Actual code changes detected in config files (rare, but possible in scripts)
  - `dependency`: Library version changes (e.g., `spring-boot 2.5.0 ‚Üí 2.7.0`)
  - `build_config`: Build settings like compiler flags, plugin versions
  - `spring_profile`: Non-production Spring profile changes
  - `config`: General configuration changes (ports, URLs, timeouts)
- **Example Matches:**
  - `pom.xml` dependency version bump ‚Üí MEDIUM
  - `application.yml` port change ‚Üí MEDIUM
  - `build.gradle` plugin update ‚Üí MEDIUM

---

#### LOW RISK Rules

```python
    # LOW RISK: Non-behavioral Changes
    if cat in ("file","table","binary_meta","archive_delta","archive_manifest","other"):
        return "low", "Non-behavioral or metadata/package change."
```

**Lines 563-564:**  
- **What it checks:** If category is any of: `file`, `table`, `binary_meta`, `archive_delta`, `archive_manifest`, `other`
- **Why LOW:** These are metadata or structural changes that don't affect runtime behavior
- **Categories Explained:**
  - `file`: File added/removed (not content change)
  - `table`: Database schema metadata
  - `binary_meta`: Binary file metadata (size, checksum)
  - `archive_delta`: JAR/ZIP file differences
  - `archive_manifest`: Manifest file changes
  - `other`: Unclassified, low-impact changes
- **Example Matches:**
  - New README file added ‚Üí LOW
  - JAR file size changed (same version) ‚Üí LOW
  - Manifest file updated ‚Üí LOW

---

```python
    # DEFAULT: Low risk if no rules match
    return "low", "Default low risk."
```

**Line 566:**  
- **Fallback:** If none of the above rules match, default to LOW risk
- **Philosophy:** "Innocent until proven guilty" - unknown changes are assumed safe but still flagged for review

---

### Stage 2: Policy Tagging

**Location:** `shared/drift_analyzer/drift_v1.py`  
**Function:** `_tag_with_policy(d: Dict[str, Any], policies: Dict[str, Any]) -> Dict[str, Any]`

After deterministic risk assessment, the system applies **policy rules** to override or refine risk levels.

#### Policy Rule Types

##### 1. `allowed_variance` (Environment-Specific Overrides)

```python
    env_allow = set(str(x).lower() for x in (policies.get("env_allow_keys") or []))
    loc_val = (d.get("locator") or {}).get("value","").lower()

    if any(tok in loc_val for tok in env_allow):
        tag, rule = "allowed_variance", "env_allow_keys"
```

**Lines 581-585:**  
- **What it does:** If the locator contains any key from `env_allow_keys` policy, mark as `allowed_variance`
- **Purpose:** Environment-specific configurations (e.g., `DEV_URL`, `QA_DB`) are expected to differ between branches
- **Example Policy (`config/policy.yml`):**
  ```yaml
  env_allow_keys:
    - "environment"
    - "env"
    - "stage"
    - "qa"
    - "dev"
  ```
- **Example Match:**
  - `application.yml.database.environment` ‚Üí `allowed_variance`
  - `config.properties.env.name` ‚Üí `allowed_variance`

---

##### 2. `invariant_breach` (Hard-Coded Constraints)

```python
    for inv in (policies.get("invariants") or []):
        lc = str(inv.get("locator_contains","")).lower()
        if lc and lc in loc_val:
            forbid = set(inv.get("forbid_values", []))
            if d.get("new") in forbid:
                tag, rule = "invariant_breach", (inv.get("name") or "invariant")
```

**Lines 587-592:**  
- **What it does:** Check if the new value violates a hard-coded invariant
- **Purpose:** Prevent critical misconfigurations (e.g., enabling debug mode in production)
- **Example Policy:**
  ```yaml
  invariants:
    - name: "no_debug_in_prod"
      locator_contains: "debug"
      forbid_values: ["true", "enabled"]
    - name: "require_ssl"
      locator_contains: "ssl.enabled"
      forbid_values: ["false"]
  ```
- **Example Breach:**
  - `application.yml.logging.debug` changed from `false` to `true` ‚Üí `invariant_breach`
  - `security.ssl.enabled` changed from `true` to `false` ‚Üí `invariant_breach`

---

### Why Two Stages?

1. **Speed:** Deterministic rules are instant (no API calls, no LLM wait)
2. **Baseline Safety:** Provides a safety net even if LLM fails or is unavailable
3. **Policy Enforcement:** Hard constraints (invariants) cannot be overridden by LLM judgment
4. **Explainability:** Rule-based decisions are transparent and auditable

---

## 3. LLM Risk Categorization (Intelligent Brain)

### Overview

After deterministic rules provide a **baseline**, the LLM performs **contextual analysis** to:
1. Re-categorize deltas based on semantic understanding
2. Provide detailed risk explanations (`ai_review_assistant`)
3. Suggest remediation steps
4. Group related changes for better review

**Location:** `Agents/workers/triaging_routing/prompts/llm_format_prompt.py`  
**Function:** `build_llm_format_prompt()`

### LLM Prompt Engineering

#### Input to LLM

```python
def build_llm_format_prompt(
    file: str,
    deltas: List[Dict[str, Any]],
    environment: str = "production",
    policies: Dict[str, Any] = None
) -> str:
```

**Parameters:**
- `file`: The config file being analyzed (e.g., `application.yml`)
- `deltas`: List of all deltas in this file (from deterministic stage)
- `environment`: Target environment (`production`, `staging`, `dev`)
- `policies`: Policy rules for context

---

#### Prompt Structure (Line by Line)

##### Part 1: Role Definition

```python
prompt = f"""You are an expert configuration drift analyzer for a {environment} environment.
Your task is to analyze configuration changes and categorize them by risk level.
"""
```

**Purpose:** Set the LLM's persona and context-awareness. The LLM behaves differently for `production` (strict) vs `dev` (lenient).

---

##### Part 2: Categorization Guidelines

```python
### **high** (Critical - Database/Security):
- Database credentials changed (usernames, passwords, connection strings)
- Security features disabled
- Production endpoints modified
- Authentication/authorization changes
- Policy violations (invariant_breach)

### **medium** (Important - Configuration/Dependencies):
- Network configuration changes
- Dependency version changes
- Feature behavior modifications
- Performance settings adjusted

### **low** (Minor):
- Logging level changes
- Comment updates
- Minor tweaks

### **allowed_variance** (Acceptable):
- Environment-specific configuration (dev vs qa vs prod differences)
- Test suite configuration
- Build/CI pipeline settings
- Documentation changes
- Policy tag = "allowed_variance"
```

**Lines 180-203:**  
- **Purpose:** Provide explicit guidelines for risk categorization
- **Note:** These guidelines **refine** the deterministic baseline, not replace it
- **Key Differences from Deterministic:**
  - **Semantic Understanding:** LLM can understand "database connection" even without keyword `password`
  - **Context-Aware:** LLM knows that `server.port` in a load balancer config is HIGH, but in a test server config is LOW
  - **Policy-Aware:** LLM respects `invariant_breach` tags from Stage 2

---

##### Part 3: Delta List Presentation

```python
# Example of how deltas are presented to LLM:
"""
## DELTA #1
- **ID**: cfg~application.yml.server.port
- **Category**: config
- **File**: application.yml
- **Locator**: application.yml.server.port
- **Old Value**: "8080"
- **New Value**: "8090"
- **Baseline Risk**: med
- **Policy Tag**: suspect
"""
```

**Purpose:** LLM receives structured information for each delta, including:
- **Baseline Risk:** From deterministic stage (for context)
- **Policy Tag:** From policy engine (for constraint awareness)
- **Old/New Values:** For semantic analysis

---

##### Part 4: Output Format Requirements

```python
{
  "high": [
    {
      "id": "cfg~application.yml.db.password",
      "file": "application.yml",
      "locator": {"type": "keypath", "value": "application.yml.db.password"},
      "old": "oldpass123",
      "new": "newpass456",
      "why": "Database password changed - potential security risk",
      "remediation": {
        "snippet": "Use environment variables for passwords"
      },
      "ai_review_assistant": {
        "potential_risk": "Hardcoded password change can expose database credentials in Git history. If leaked, attackers can access production database with full permissions, leading to data breaches and compliance violations.",
        "suggested_action": "1. Verify the new password is stored in a secrets manager (e.g., AWS Secrets Manager, HashiCorp Vault). 2. Test database connectivity in staging with the new password. 3. Monitor database access logs for 24 hours after deployment. 4. Prepare a rollback plan in case authentication fails."
      }
    }
  ],
  "medium": [...],
  "low": [...],
  "allowed_variance": [...]
}
```

**Lines 147-167:**  
- **Purpose:** Force LLM to output structured JSON for programmatic parsing
- **Key Fields:**
  - `id`: Must match input delta ID (for traceability)
  - `why`: Single-sentence explanation of the change
  - `ai_review_assistant`: Detailed risk analysis for human reviewers
    - `potential_risk`: 2-3 sentences on "what could go wrong"
    - `suggested_action`: 4 numbered steps for verification

---

### LLM Intelligence Features

#### 1. Semantic Understanding

**Example: Port Change Context**

**Deterministic Output:** `server.port` change ‚Üí MEDIUM (always)

**LLM Output:**
- **Scenario A:** `application.yml.server.port` in a web service
  - LLM: HIGH ‚Üí "Port change can break load balancer routing, causing 100% traffic loss"
- **Scenario B:** `test-config.yml.server.port` in a test suite
  - LLM: LOW ‚Üí "Test server port change has no production impact"

**Why?** LLM understands the **context** (file name `test-config.yml`) and **business impact** (test vs production).

---

#### 2. Typo Detection

**Example: Suspicious Value**

```json
{
  "old": "https://api-ptg-sqa3.company.com",
  "new": "https://api-08v-beta2.company.com"
}
```

**Deterministic Output:** MEDIUM (URL change)

**LLM Output:** HIGH + reasoning:
```
"potential_risk": "The new URL 'api-08v-beta2' does not follow the standard naming convention 'ptg-sqa3'. Cross-referencing with other config files shows '08v' is never used elsewhere, while 'ptg-sqa3' appears 47 times. This is likely a typo that will result in 404 errors, breaking payment gateway integration and causing transaction failures."
```

**Why?** LLM performs **pattern analysis** across multiple files to detect anomalies.

---

#### 3. Dependency Chain Analysis

**Example: Version Bump**

```json
{
  "file": "pom.xml",
  "old": "spring-boot:2.5.0",
  "new": "spring-boot:3.0.0"
}
```

**Deterministic Output:** MEDIUM (dependency change)

**LLM Output:** HIGH + reasoning:
```
"potential_risk": "Spring Boot 3.0 is a major version upgrade with breaking changes. This includes Java 17 requirement, Jakarta EE namespace changes, and deprecated API removals. Without verifying all downstream dependencies and testing integration points, this change can cause compilation failures, runtime errors, and service crashes."

"suggested_action": "1. Verify all dependencies are compatible with Spring Boot 3.0. 2. Run full regression test suite. 3. Check for deprecated API usage in codebase. 4. Deploy to staging for 48-hour soak test before production."
```

**Why?** LLM understands **semantic versioning** (major vs minor) and **ecosystem dependencies** (Spring Boot impacts many libraries).

---

#### 4. Policy-Aware Overrides

**Example: Allowed Variance**

```json
{
  "locator": "application.yml.environment.name",
  "old": "production",
  "new": "staging",
  "policy": {"tag": "allowed_variance", "rule": "env_allow_keys"}
}
```

**Deterministic Output:** HIGH ‚Üí lowered to `allowed_variance` by policy

**LLM Output:** `allowed_variance` + reasoning:
```
"rationale": "Environment name is an expected difference between branches. This change is explicitly allowed by policy 'env_allow_keys'."
```

**Why?** LLM respects policy overrides and explains them in human terms.

---

## 4. Confidence Score Calculation

### Overview

After LLM categorization, the **ConfidenceScorer** calculates a 0-100 score to determine if changes can be auto-merged, require review, or must be blocked.

**Location:** `Agents/workers/certification/confidence_scorer.py`  
**Class:** `ConfidenceScorer`

### Scoring Formula

```
Final Score = Baseline + Deductions + Bonuses (clamped to 0-100)

Components:
  1. Base Score:              +100  (full confidence)
  2. Policy Deductions:       -5 to -30 per violation
  3. Risk Deductions:         -55 to -80 for medium/high/critical
  4. Blast Radius Penalty:    -5 to -50 (impact magnitude)
  5. History Adjustment:      -20 to +10 (learning from past)
  6. LLM Safety Adjustment:   -20 to +15 (contextual reasoning)
  7. Context Bonus:           +0 to +25 (MR quality)
  8. Evidence Adjustments:    -20 to +20 (testing evidence)
  9. Historical Bonus:        +0 to +10 (approval patterns)
```

---

### Component Breakdown (Line by Line)

#### 1. Base Score

```python
# Start with base score (Baseline = 100, full confidence)
score = 100
components = {
    "base_score": 100,
    "policy_deductions": 0,
    "risk_deductions": 0,
    # ... (initialized to 0)
}
```

**Lines 100-112:**  
- **Purpose:** Start with perfect confidence (100) and subtract penalties
- **Philosophy:** "Guilty until proven innocent" - changes are risky until validated

---

#### 2. Policy Deductions

```python
def _calculate_policy_deductions(self, violations: List[Dict[str, Any]]) -> int:
    """Calculate deductions for policy violations"""
    deduction = 0
    
    for violation in violations:
        severity = violation.get('severity', 'medium').lower()
        if severity == 'critical':
            deduction += 30
        elif severity == 'high':
            deduction += 15
        elif severity == 'medium':
            deduction += 5
    
    return deduction
```

**Lines 202-215:**  
- **What it does:** Sum penalties for all policy violations
- **Penalty Tiers:**
  - **Critical:** -30 points (e.g., `invariant_breach` for disabling SSL)
  - **High:** -15 points (e.g., missing required approval)
  - **Medium:** -5 points (e.g., minor policy non-compliance)
- **Example Calculation:**
  - 1 critical violation ‚Üí -30 ‚Üí Score = 70
  - 2 high violations ‚Üí -30 ‚Üí Score = 70
  - 3 medium violations ‚Üí -15 ‚Üí Score = 85

---

#### 3. Risk Deductions (CRITICAL RULE)

```python
def _calculate_risk_deduction_from_counts(
    self, 
    high_count: int, 
    medium_count: int, 
    low_count: int, 
    critical_count: int = 0
) -> int:
    """
    RIGOROUS SCORING: Calculate risk deduction based on actual counts.
    
    CRITICAL RULE: Any medium/high/critical drift ‚Üí score MUST be < 50
    """
    deduction = 0
    
    # CRITICAL: Any critical/high/medium = instant BLOCK (score < 50)
    if critical_count > 0:
        # Critical risk: Score must be < 50
        deduction = 80  # Even with base 100, score = 20 (well below 50)
    
    elif high_count > 0:
        # High risk: Score must be < 50
        deduction = 60  # Even with base 100, score = 40 (below 50)
    
    elif medium_count > 0:
        # Medium risk: Score must be < 50 (EVEN 1 MEDIUM = BLOCK)
        deduction = 55  # Even with base 100, score = 45 (below 50)
    
    else:
        # Only low risk items: Deduct based on quantity (can still score > 50)
        if low_count > 0:
            # Low risk: -2 per item (more penalty than before)
            # With 10 low items: -20 ‚Üí score = 80 (still > 50)
            # With 25 low items: -50 ‚Üí score = 50 (exactly 50)
            # With 30+ low items: score < 50
            low_deduction = low_count * 2
            deduction = min(low_deduction, 60)  # Cap at -60
    
    return deduction
```

**Lines 232-269:**  
- **CRITICAL RULE:** **ANY medium, high, or critical risk item ‚Üí Score MUST be < 50 ‚Üí BLOCK_MERGE**
- **Why this rule?** Medium+ risks require human judgment; automation cannot safely approve them
- **Penalty Details:**
  - **Critical:** -80 points ‚Üí Score = 20 (hard block)
  - **High:** -60 points ‚Üí Score = 40 (hard block)
  - **Medium:** -55 points ‚Üí Score = 45 (hard block, even for 1 medium item)
  - **Low:** -2 per item (variable, can still score > 50)
    - 10 low items ‚Üí -20 ‚Üí Score = 80 (AUTO_MERGE possible)
    - 25 low items ‚Üí -50 ‚Üí Score = 50 (HUMAN_REVIEW)
    - 30+ low items ‚Üí Score < 50 (BLOCK_MERGE)
- **Example:**
  - 1 medium + 50 low items ‚Üí -55 ‚Üí Score = 45 ‚Üí BLOCK (medium takes priority)
  - 0 medium + 10 low items ‚Üí -20 ‚Üí Score = 80 ‚Üí AUTO_MERGE (if no other deductions)

---

#### 4. Blast Radius Penalty (Impact Magnitude)

```python
def _calculate_blast_radius_penalty(self, blast_radius: Dict[str, Any]) -> int:
    """
    Calculate Impact Magnitude Penalty based on blast radius.
    
    Inspired by PTG Adapter example: "5 drifted references in a critical adapter YAML 
    (single change fan-out, high blast radius)" ‚Üí -25 points
    """
    files_affected = blast_radius.get('files_affected', 1)
    critical_files = blast_radius.get('critical_files', 0)
    downstream_services = blast_radius.get('downstream_services', [])
    scope = blast_radius.get('scope', 'low').lower()
    
    penalty = 0
    
    # Base penalty from scope
    scope_penalties = {
        'critical': 30,  # Critical infrastructure changes
        'high': 25,      # High fan-out (PTG example)
        'medium': 15,    # Moderate impact
        'low': 5         # Limited scope
    }
    penalty += scope_penalties.get(scope, 15)
    
    # Additional penalty for multiple files
    if files_affected > 5:
        penalty += 10
    elif files_affected > 3:
        penalty += 5
    
    # Additional penalty for critical files (auth, database, etc)
    if critical_files > 0:
        penalty += critical_files * 5
    
    # Additional penalty for downstream service dependencies
    if downstream_services:
        penalty += min(len(downstream_services) * 3, 15)
    
    return min(penalty, 50)  # Cap at 50
```

**Lines 296-341:**  
- **Purpose:** Penalize changes with wide impact (many files, critical systems, downstream dependencies)
- **Inspiration:** Real-world incident where changing 1 adapter config file broke 5 downstream services
- **Penalty Calculation:**
  - **Base Scope:**
    - Critical infrastructure (auth, database) ‚Üí -30
    - High fan-out (5+ affected services) ‚Üí -25
    - Moderate impact (2-4 services) ‚Üí -15
    - Low impact (1 service) ‚Üí -5
  - **Multiple Files:**
    - 5+ files ‚Üí +10 penalty
    - 3-5 files ‚Üí +5 penalty
  - **Critical Files:**
    - +5 per critical file (e.g., auth config, DB config)
  - **Downstream Services:**
    - +3 per dependent service (max +15)
- **Example:**
  - Scope: high (25) + 6 files (10) + 2 critical files (10) + 3 downstream (9) = 54 ‚Üí capped at 50
  - Final: -50 points

---

#### 5. History Adjustment (Learning from Past)

```python
def _calculate_history_adjustment(self, historical_analysis: Dict[str, Any]) -> int:
    """
    Calculate History Score adjustment based on past behavior in this area.
    
    Inspired by PTG Adapter: "Prior alias/route typo caused outage in this area 
    ‚Äî slight distrust" ‚Üí -5 points
    """
    past_failures = historical_analysis.get('past_failures', 0)
    past_successes = historical_analysis.get('past_successes', 0)
    outage_history = historical_analysis.get('outage_history', False)
    trust_level = historical_analysis.get('trust_level', 0.5)
    
    adjustment = 0
    
    # Penalty for past failures
    if outage_history:
        adjustment -= 20  # Significant distrust if past outages
    elif past_failures > 0:
        adjustment -= min(past_failures * 5, 15)  # -5 per failure, max -15
    
    # Bonus for clean history
    if past_successes > 5 and past_failures == 0:
        adjustment += 10  # Trust bonus for clean track record
    elif past_successes > 0:
        adjustment += min(past_successes * 2, 5)
    
    # Trust level adjustment (-10 to +10)
    if trust_level < 0.3:
        adjustment -= 10  # Low trust
    elif trust_level > 0.8:
        adjustment += 10  # High trust
    
    return max(-20, min(adjustment, 10))  # Range: -20 to +10
```

**Lines 343-384:**  
- **Purpose:** Learn from historical patterns in this config area
- **Use Case:** If a specific config file has caused outages before, be more cautious
- **Adjustment Logic:**
  - **Outage History:** -20 (e.g., this adapter config caused production down last quarter)
  - **Past Failures:** -5 per failure (max -15)
  - **Clean Track Record:** +10 (5+ successes, 0 failures)
  - **Trust Level:** -10 (low trust) to +10 (high trust)
- **Example:**
  - Config file has 1 past outage ‚Üí -20
  - Same file has 10 successful changes ‚Üí +10
  - Net: -10 points

---

#### 6. LLM Safety Adjustment (Contextual Reasoning)

```python
def _calculate_llm_safety_adjustment(
    self, 
    safety_probability: float,  # 0.0 to 1.0
    anomaly_score: float         # 0.0 to 1.0
) -> int:
    """
    Calculate LLM Safety Probability adjustment from contextual reasoning.
    
    Inspired by PTG Adapter: "LLM sees ptg-sqa3 used many times, 08v-beta2 never; 
    infers likely typo ‚Äî very low safety" ‚Üí +4 points (scaled)
    """
    adjustment = 0
    
    # Penalty for low safety probability (likely typo/error)
    if safety_probability < 0.3:
        # Very unsafe: -20 points
        adjustment -= 20
    elif safety_probability < 0.5:
        # Somewhat unsafe: -10 points
        adjustment -= 10
    elif safety_probability > 0.8:
        # Very safe: +15 points
        adjustment += 15
    elif safety_probability > 0.6:
        # Somewhat safe: +5 points
        adjustment += 5
    
    # Additional penalty for anomalies (unknown service IDs, etc)
    if anomaly_score > 0.7:
        adjustment -= 15  # High anomaly detected
    elif anomaly_score > 0.5:
        adjustment -= 10  # Moderate anomaly
    elif anomaly_score > 0.3:
        adjustment -= 5   # Low anomaly
    
    return max(-20, min(adjustment, 15))  # Range: -20 to +15
```

**Lines 386-421:**  
- **Purpose:** Incorporate LLM's probabilistic judgment on change safety
- **Inputs:**
  - **safety_probability:** LLM's confidence this change is safe (0.0 = dangerous, 1.0 = safe)
  - **anomaly_score:** LLM's detection of unusual patterns (0.0 = normal, 1.0 = anomalous)
- **Adjustment Logic:**
  - **Safety Probability:**
    - < 0.3 (very unsafe) ‚Üí -20 (e.g., LLM detects typo)
    - < 0.5 (unsafe) ‚Üí -10
    - > 0.8 (very safe) ‚Üí +15 (e.g., LLM validates pattern)
    - > 0.6 (safe) ‚Üí +5
  - **Anomaly Score:**
    - > 0.7 (high anomaly) ‚Üí -15 (e.g., unknown service ID)
    - > 0.5 (moderate) ‚Üí -10
    - > 0.3 (low) ‚Üí -5
- **Example:**
  - LLM detects likely typo (safety = 0.2) ‚Üí -20
  - LLM detects unknown service ID (anomaly = 0.8) ‚Üí -15
  - Total: -35 points

---

#### 7. Context Bonus (MR Quality)

```python
def _calculate_context_bonus(self, mr_context: Dict[str, Any]) -> int:
    """
    Calculate Context Bonus based on MR quality and documentation.
    
    Inspired by PTG Adapter: "No MR tag like [rename-adapter], no Jira rename plan, 
    no rollback note" ‚Üí 0 points
    """
    bonus = 0
    
    # MR tags present ([rename], [feature], etc)
    if mr_context.get('has_mr_tags'):
        bonus += 5
    
    # Jira ticket linked
    if mr_context.get('has_jira_link'):
        bonus += 5
    
    # Rollback plan documented
    if mr_context.get('has_rollback_plan'):
        bonus += 10
    
    # Test evidence provided
    if mr_context.get('has_test_evidence'):
        bonus += 5
    
    # Description quality
    desc_quality = mr_context.get('description_quality', 'low').lower()
    if desc_quality == 'high':
        bonus += 5
    elif desc_quality == 'medium':
        bonus += 2
    
    return min(bonus, 25)  # Cap at 25
```

**Lines 423-464:**  
- **Purpose:** Reward well-documented changes with clear intent and safety planning
- **Bonus Details:**
  - **MR Tags:** +5 (e.g., `[rename]`, `[feature-toggle]`, `[hotfix]`)
  - **Jira Link:** +5 (traceable to requirements/bug)
  - **Rollback Plan:** +10 (shows risk awareness)
  - **Test Evidence:** +5 (proven in staging)
  - **Description Quality:**
    - High (detailed, clear) ‚Üí +5
    - Medium (some details) ‚Üí +2
    - Low (minimal) ‚Üí 0
- **Example:**
  - MR has all: +5 (tag) +5 (jira) +10 (rollback) +5 (tests) +5 (desc) = 30 ‚Üí capped at 25

---

#### 8. Evidence Adjustments

```python
def _calculate_evidence_adjustment(self, evidence: Dict[str, Any]) -> int:
    """Calculate adjustment based on evidence"""
    adjustment = 0
    
    found = evidence.get('found', [])
    missing = evidence.get('missing', [])
    
    # Bonus if all evidence present
    if found and not missing:
        adjustment += 20
    # Penalty if evidence missing
    elif missing:
        adjustment -= 20
    
    return adjustment
```

**Lines 271-285:**  
- **Purpose:** Reward changes with complete testing evidence
- **Evidence Types:**
  - Unit tests passed
  - Integration tests passed
  - Staging deployment successful
  - Smoke tests passed
- **Adjustment:**
  - All evidence present ‚Üí +20
  - Missing evidence ‚Üí -20

---

#### 9. Final Score Calculation

```python
# Apply all components
score = 100
score -= policy_deduction
score -= risk_deduction
score -= blast_radius_penalty
score += history_adjustment
score += llm_safety_adjustment
score += context_bonus
score += evidence_adjustment
score += historical_bonus

# Clamp to 0-100
score = max(0, min(100, score))

# Determine decision
decision = self._determine_decision(score, environment, high_risk_count, medium_risk_count, critical_count)
```

**Lines 100-186:**  
- **Process:** Sum all components, clamp to 0-100, determine decision
- **Example Calculation:**
  ```
  Base:              +100
  Policy:            -15  (1 high violation)
  Risk:              -55  (1 medium item)
  Blast Radius:      -25  (high scope)
  History:           -5   (1 past failure)
  LLM Safety:        +0   (neutral)
  Context:           +10  (good MR)
  Evidence:          -20  (missing tests)
  Historical:        +0
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  TOTAL:             -10
  Final Score:       max(0, 100 - 10) = 90... BUT medium item present
  Decision:          BLOCK_MERGE (CRITICAL RULE overrides score)
  ```

---

## 5. Decision Thresholds

### CRITICAL DECISION RULE

```python
def _determine_decision(
    self, 
    score: int, 
    environment: str, 
    high_risk_count: int = 0, 
    medium_risk_count: int = 0, 
    critical_count: int = 0
) -> str:
    """
    RIGOROUS DECISION: Determine certification decision.
    
    CRITICAL RULE: Medium/High/Critical risk ‚Üí ALWAYS BLOCK
    """
    # CRITICAL: Any medium/high/critical risk ‚Üí IMMEDIATE BLOCK
    if critical_count > 0 or high_risk_count > 0 or medium_risk_count > 0:
        return "BLOCK_MERGE"
    
    # Only low/allowed_variance items: Use score-based decision
    # (Environment thresholds apply here)
```

**Lines 472-482:**  
- **RULE:** **ANY medium, high, or critical risk item ‚Üí IMMEDIATE BLOCK_MERGE** (regardless of score)
- **Why?** Medium+ risks are too complex for automated approval; require human judgment
- **Example:**
  - Score = 95 (excellent) BUT 1 medium risk item ‚Üí BLOCK_MERGE
  - Score = 95 (excellent) AND 0 medium+ items ‚Üí AUTO_MERGE (if environment allows)

---

### Environment-Based Thresholds

#### Production Environment

```python
if environment == "production":
    if score >= 85:
        return "AUTO_MERGE"     # ‚úÖ High confidence, safe to merge
    elif score >= 60:
        return "HUMAN_REVIEW"   # ‚ö†Ô∏è Moderate confidence, review needed
    else:
        return "BLOCK_MERGE"    # üö´ Low confidence, block
```

**Lines 484-490:**  
- **Thresholds:**
  - **85-100:** AUTO_MERGE ‚úÖ (only low/allowed_variance, score high)
  - **60-84:** HUMAN_REVIEW ‚ö†Ô∏è (borderline, needs eyes)
  - **0-59:** BLOCK_MERGE üö´ (too risky)
- **Rationale:** Production requires highest confidence

---

#### Staging/Pre-Production Environment

```python
elif environment in ["staging", "pre-production"]:
    if score >= 75:
        return "AUTO_MERGE"     # ‚úÖ Moderate confidence acceptable
    elif score >= 50:
        return "HUMAN_REVIEW"   # ‚ö†Ô∏è Lower bar for review
    else:
        return "BLOCK_MERGE"    # üö´ Still block very low scores
```

**Lines 491-497:**  
- **Thresholds:**
  - **75-100:** AUTO_MERGE ‚úÖ (10 points more lenient than prod)
  - **50-74:** HUMAN_REVIEW ‚ö†Ô∏è (10 points more lenient than prod)
  - **0-49:** BLOCK_MERGE üö´
- **Rationale:** Staging is for experimentation, but still needs safety

---

#### Development/Testing Environment

```python
else:  # development, testing
    if score >= 65:
        return "AUTO_MERGE"     # ‚úÖ Lower bar for dev
    elif score >= 50:
        return "HUMAN_REVIEW"   # ‚ö†Ô∏è Stricter than before (was 40)
    else:
        return "BLOCK_MERGE"    # üö´ Even dev has limits
```

**Lines 498-504:**  
- **Thresholds:**
  - **65-100:** AUTO_MERGE ‚úÖ (20 points more lenient than prod)
  - **50-64:** HUMAN_REVIEW ‚ö†Ô∏è (still requires >= 50 for safety)
  - **0-49:** BLOCK_MERGE üö´
- **Rationale:** Dev should be fast, but not reckless

---

### Decision Matrix

| Score | Production | Staging | Development |
|-------|-----------|---------|-------------|
| 100-85 | ‚úÖ AUTO_MERGE | ‚úÖ AUTO_MERGE | ‚úÖ AUTO_MERGE |
| 84-75 | ‚ö†Ô∏è HUMAN_REVIEW | ‚úÖ AUTO_MERGE | ‚úÖ AUTO_MERGE |
| 74-65 | ‚ö†Ô∏è HUMAN_REVIEW | ‚ö†Ô∏è HUMAN_REVIEW | ‚úÖ AUTO_MERGE |
| 64-60 | ‚ö†Ô∏è HUMAN_REVIEW | ‚ö†Ô∏è HUMAN_REVIEW | ‚ö†Ô∏è HUMAN_REVIEW |
| 59-50 | üö´ BLOCK_MERGE | ‚ö†Ô∏è HUMAN_REVIEW | ‚ö†Ô∏è HUMAN_REVIEW |
| 49-0 | üö´ BLOCK_MERGE | üö´ BLOCK_MERGE | üö´ BLOCK_MERGE |

**OVERRIDE:** If ANY medium/high/critical risk item exists ‚Üí üö´ BLOCK_MERGE (all environments)

---

## 6. Real-World Examples

### Example 1: Simple Port Change (Auto-Merge)

**Scenario:**
- File: `application.yml`
- Change: `server.port` from `8080` to `8090`
- Environment: `development`

**Risk Assessment:**
```
Deterministic:
  - Category: config
  - Risk Level: med (behavioral change)
  - Policy Tag: suspect

LLM:
  - Categorization: low
  - Reasoning: "Port change in dev environment with no downstream dependencies"
```

**Confidence Score:**
```
Base:              +100
Policy:            0    (no violations)
Risk:              -2   (1 low item √ó 2)
Blast Radius:      -5   (low scope, 1 file)
History:           +0   (no past issues)
LLM Safety:        +5   (safe pattern)
Context:           +5   (has MR tag)
Evidence:          +0   (no tests required for dev)
Historical:        +0
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL:             +103 ‚Üí clamped to 100
Decision:          AUTO_MERGE ‚úÖ (dev: 100 >= 65)
```

**Result:** ‚úÖ **AUTO_MERGE** - Low risk, dev environment, high confidence

---

### Example 2: Database Credential Change (Block)

**Scenario:**
- File: `application-prod.yml`
- Change: `spring.datasource.password` from `oldpass123` to `newpass456`
- Environment: `production`

**Risk Assessment:**
```
Deterministic:
  - Category: config
  - Risk Level: high (keyword: "password")
  - Policy Tag: suspect

LLM:
  - Categorization: high
  - Reasoning: "Hardcoded password in production config - security risk"
  - AI Assistant: "If leaked to Git history, attackers can access production database"
```

**Confidence Score:**
```
Base:              +100
Policy:            0    (no violations)
Risk:              -60  (1 high item)
Blast Radius:      -30  (critical file: database)
History:           +0   (no past issues)
LLM Safety:        -10  (unsafe: hardcoded password)
Context:           +0   (no MR documentation)
Evidence:          -20  (no test evidence)
Historical:        +0
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL:             -20
Final Score:       max(0, 100 - 20) = 80
BUT: high_risk_count = 1 ‚Üí BLOCK_MERGE (CRITICAL RULE)
```

**Result:** üö´ **BLOCK_MERGE** - High risk item detected (CRITICAL RULE overrides score)

**Explanation:** "Score: 80/100. üö´ BLOCKED: 1 high risk item(s) detected (rigorous policy: medium+ = BLOCK). Pipeline blocked due to medium/high/critical risk items."

---

### Example 3: PTG Adapter Typo (Block with Intelligence)

**Scenario:**
- File: `ptg-adapter-config.yml`
- Change: `payment.gateway.url` from `https://apigateway-ptg-sqa3.ebiz.verizon.com` to `https://apigateway-d08v-beta2.ebiz.verizon.com`
- Environment: `production`

**Risk Assessment:**
```
Deterministic:
  - Category: config
  - Risk Level: med (URL change)
  - Policy Tag: suspect

LLM:
  - Categorization: high
  - Reasoning: "Anomaly detected: 'ptg-sqa3' used 47 times, 'd08v-beta2' never used. Likely typo."
  - AI Assistant: "This will cause 404 errors on payment gateway, breaking all transactions"
  - Anomaly Score: 0.9 (high)
  - Safety Probability: 0.2 (very unsafe)
```

**Confidence Score:**
```
Base:              +100
Policy:            0    (no violations)
Risk:              -60  (LLM upgraded to high)
Blast Radius:      -25  (high scope: 5 services depend on this)
History:           -5   (prior typo in this area)
LLM Safety:        -35  (safety: 0.2 ‚Üí -20, anomaly: 0.9 ‚Üí -15)
Context:           +0   (no MR documentation)
Evidence:          -20  (no test evidence)
Historical:        +0
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL:             -45
Final Score:       max(0, 100 - 45) = 55
BUT: high_risk_count = 1 ‚Üí BLOCK_MERGE (CRITICAL RULE)
```

**Result:** üö´ **BLOCK_MERGE** - High risk item detected (LLM intelligence prevented catastrophic typo)

**Why this is impressive:**
- **Deterministic:** Would have only flagged as MEDIUM (URL change)
- **LLM:** Detected anomaly by cross-referencing 47 other config files
- **Outcome:** Prevented production payment outage

---

### Example 4: Environment-Specific Change (Allowed Variance)

**Scenario:**
- File: `application.yml`
- Change: `environment.name` from `production` to `staging`
- Environment: `staging`

**Risk Assessment:**
```
Deterministic:
  - Category: config
  - Risk Level: med (config change)
  - Policy Tag: allowed_variance (env_allow_keys rule)

LLM:
  - Categorization: allowed_variance
  - Reasoning: "Expected environment-specific difference, policy allows this"
```

**Confidence Score:**
```
Base:              +100
Policy:            0    (no violations, allowed by policy)
Risk:              0    (allowed_variance counts as 0 risk)
Blast Radius:      0    (allowed variance exempted)
History:           +0
LLM Safety:        +0
Context:           +0
Evidence:          +0
Historical:        +0
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL:             0
Final Score:       100
Decision:          AUTO_MERGE ‚úÖ (staging: 100 >= 75)
```

**Result:** ‚úÖ **AUTO_MERGE** - Allowed by policy, zero risk

---

### Example 5: Many Low-Risk Changes (Human Review)

**Scenario:**
- File: `application.yml`
- Changes: 20 logging level changes (debug ‚Üí info)
- Environment: `production`

**Risk Assessment:**
```
Deterministic:
  - Category: config (√ó20)
  - Risk Level: low (logging changes)
  - Policy Tag: suspect

LLM:
  - Categorization: low (√ó20)
  - Reasoning: "Non-behavioral logging changes, no service impact"
```

**Confidence Score:**
```
Base:              +100
Policy:            0    (no violations)
Risk:              -40  (20 low items √ó 2 = 40)
Blast Radius:      -5   (low scope)
History:           +0
LLM Safety:        +0
Context:           +0
Evidence:          +0
Historical:        +0
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL:             -45
Final Score:       max(0, 100 - 45) = 55
Decision:          HUMAN_REVIEW ‚ö†Ô∏è (prod: 55 < 85, but >= 60)
```

**Result:** ‚ö†Ô∏è **HUMAN_REVIEW** - Many changes (even low risk) require human verification

**Why not AUTO_MERGE?**
- **Volume:** 20 changes are too many to auto-approve
- **Score:** 55 < 85 (production threshold)
- **Philosophy:** Bulk changes need eyes, even if individually low risk

---

## 7. Code Walkthrough

### File Structure

```
project_root/
‚îú‚îÄ‚îÄ shared/
‚îÇ   ‚îî‚îÄ‚îÄ drift_analyzer/
‚îÇ       ‚îî‚îÄ‚îÄ drift_v1.py                    # Deterministic risk assessment
‚îú‚îÄ‚îÄ Agents/
‚îÇ   ‚îî‚îÄ‚îÄ workers/
‚îÇ       ‚îú‚îÄ‚îÄ triaging_routing/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ prompts/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ llm_format_prompt.py   # LLM prompt engineering
‚îÇ       ‚îî‚îÄ‚îÄ certification/
‚îÇ           ‚îú‚îÄ‚îÄ confidence_scorer.py       # Confidence scoring
‚îÇ           ‚îî‚îÄ‚îÄ certification_engine_agent.py  # Orchestration
‚îî‚îÄ‚îÄ config/
    ‚îî‚îÄ‚îÄ policy.yml                         # Policy rules
```

---

### Execution Flow (Full Trace)

#### Step 1: Drift Detection
**File:** `Agents/workers/drift_detector/drift_detector_agent.py`

```python
# Compare golden vs drift branch
deltas = compare_branches(golden_branch, drift_branch)

# Example delta:
delta = {
    "id": "cfg~application.yml.db.password",
    "category": "config",
    "file": "application.yml",
    "locator": {"type": "keypath", "value": "application.yml.db.password"},
    "old": "oldpass",
    "new": "newpass"
}
```

---

#### Step 2: Deterministic Risk Assessment
**File:** `shared/drift_analyzer/drift_v1.py`

```python
# Line 542-566
def _risk_level_and_reason(delta):
    cat = delta.get("category")  # "config"
    file = delta.get("file")      # "application.yml"
    loc = delta.get("locator").get("value")  # "application.yml.db.password"
    
    # Line 551: Check for "password" keyword
    if "password" in loc:
        return "high", "Sensitive credential changed"
    
    # Returns: ("high", "Sensitive credential changed")

# Add to delta:
delta["risk_level"] = "high"
delta["risk_reason"] = "Sensitive credential changed"
```

---

#### Step 3: Policy Tagging
**File:** `shared/drift_analyzer/drift_v1.py`

```python
# Line 573-595
def _tag_with_policy(delta, policies):
    loc_val = delta.get("locator").get("value")  # "application.yml.db.password"
    
    # Check env_allow_keys (Line 584)
    env_allow = policies.get("env_allow_keys", [])  # ["environment", "env"]
    if any(tok in loc_val for tok in env_allow):
        delta["policy"] = {"tag": "allowed_variance", "rule": "env_allow_keys"}
        return delta
    
    # "password" not in env_allow_keys ‚Üí tag as "suspect"
    delta["policy"] = {"tag": "suspect", "rule": ""}
    return delta

# Delta now has:
delta["policy"] = {"tag": "suspect", "rule": ""}
```

---

#### Step 4: LLM Categorization
**File:** `Agents/workers/triaging_routing/prompts/llm_format_prompt.py`

```python
# Build prompt for LLM (Lines 11-229)
prompt = build_llm_format_prompt(
    file="application.yml",
    deltas=[delta],
    environment="production",
    policies=policies
)

# Prompt includes:
"""
## DELTA #1
- **ID**: cfg~application.yml.db.password
- **Old Value**: "oldpass"
- **New Value**: "newpass"
- **Baseline Risk**: high
- **Policy Tag**: suspect

Categorize this delta into: high, medium, low, or allowed_variance
"""

# Send to Claude LLM
response = claude_api.call(prompt)

# LLM response:
{
  "high": [
    {
      "id": "cfg~application.yml.db.password",
      "file": "application.yml",
      "locator": {"type": "keypath", "value": "application.yml.db.password"},
      "old": "oldpass",
      "new": "newpass",
      "why": "Hardcoded database password changed - security risk",
      "ai_review_assistant": {
        "potential_risk": "Hardcoded password in Git history can be exploited...",
        "suggested_action": "1. Move to secrets manager. 2. Test connectivity..."
      }
    }
  ],
  "medium": [],
  "low": [],
  "allowed_variance": []
}
```

---

#### Step 5: Confidence Score Calculation
**File:** `Agents/workers/certification/confidence_scorer.py`

```python
# Extract risk counts from LLM response
high_count = len(response["high"])  # 1
medium_count = len(response["medium"])  # 0
low_count = len(response["low"])  # 0

# Call ConfidenceScorer (Line 72-200)
scorer = ConfidenceScorer()
result = scorer.calculate(
    policy_violations=[],  # No policy violations
    risk_level="high",
    environment="production",
    high_risk_count=1,
    medium_risk_count=0,
    low_risk_count=0
)

# INSIDE calculate():

# Line 100: Start with base score
score = 100

# Line 115: Policy deductions (none)
policy_deduction = 0
score = 100

# Line 125: Risk deductions (Line 232-269)
risk_deduction = _calculate_risk_deduction_from_counts(
    high_count=1, medium_count=0, low_count=0
)
# Returns: 60 (because high_count > 0)
score = 100 - 60 = 40

# Line 186: Determine decision (Line 472-504)
if high_count > 0:  # TRUE
    return "BLOCK_MERGE"

# Final result:
result = ConfidenceScore(
    score=40,
    decision="BLOCK_MERGE",
    explanation="Score: 40/100. BLOCKED: 1 high risk item detected",
    confidence_level="LOW"
)
```

---

#### Step 6: Certification Decision
**File:** `Agents/workers/certification/certification_engine_agent.py`

```python
# Line 639-670
def make_certification_decision(confidence_score: int, environment: str):
    if confidence_score >= 85:  # 40 >= 85? NO
        return {"decision": "AUTO_MERGE", "action": "merge"}
    elif confidence_score >= 60:  # 40 >= 60? NO
        return {"decision": "HUMAN_REVIEW", "action": "review_required"}
    else:
        return {"decision": "BLOCK_MERGE", "action": "block"}

# Returns:
{
    "decision": "BLOCK_MERGE",
    "action": "block",
    "threshold": 85,
    "score": 40
}
```

---

## 8. LLM Integration

### LLM Provider: Claude (Anthropic)

**Model:** Claude 3.5 Sonnet (default) / Claude 3 Haiku (faster, cheaper)

### API Call Flow

```python
# Pseudo-code for LLM call
import anthropic

client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

# Build prompt (from llm_format_prompt.py)
prompt = build_llm_format_prompt(file, deltas, environment, policies)

# Call Claude API
response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=8000,
    temperature=0.0,  # Deterministic output
    messages=[
        {"role": "user", "content": prompt}
    ]
)

# Parse JSON response (4-tier parsing)
llm_output = parse_llm_response(response.content[0].text)
```

---

### Prompt Engineering Best Practices

1. **Structured Input:** Deltas presented in numbered list with clear labels
2. **Explicit Guidelines:** Categorization rules spelled out (high/medium/low/allowed)
3. **Required Fields:** Enforce JSON structure with mandatory fields (`id`, `old`, `new`, `why`)
4. **Error Handling:** 4-tier JSON parsing for malformed responses
5. **Fallback:** Rule-based categorization if LLM fails after retries

---

### LLM Fallback Logic

**File:** (Implemented in triaging agent, not shown in search results)

```python
def categorize_with_fallback(deltas, llm_output):
    if llm_output is None:  # LLM failed after retries
        # Fallback to rule-based categorization
        for delta in deltas:
            loc = delta["locator"]["value"].lower()
            if "password" in loc or "secret" in loc:
                category = "high"
            elif "port" in loc or "url" in loc:
                category = "medium"
            elif "logging" in loc or "debug" in loc:
                category = "low"
            else:
                category = "low"
            
            delta["llm_category"] = category
            delta["llm_fallback"] = True
    else:
        # Use LLM output
        for category, items in llm_output.items():
            for item in items:
                # Match by ID
                delta = find_delta_by_id(deltas, item["id"])
                delta["llm_category"] = category
                delta["llm_fallback"] = False
```

---

## 9. Testing & Verification

### Test Cases

#### Test 1: High Risk - Database Credential

**Input:**
```json
{
  "deltas": [
    {
      "id": "cfg~app.yml.db.password",
      "locator": {"value": "application.yml.database.password"},
      "old": "oldpass",
      "new": "newpass"
    }
  ],
  "environment": "production"
}
```

**Expected Output:**
```json
{
  "risk_level": "high",
  "confidence_score": 40,
  "decision": "BLOCK_MERGE",
  "explanation": "1 high risk item detected (rigorous policy: medium+ = BLOCK)"
}
```

**Verification:**
- ‚úÖ Deterministic: `_risk_level_and_reason()` returns `"high"` (password keyword)
- ‚úÖ LLM: Categorizes as `"high"` with security reasoning
- ‚úÖ Scorer: Deducts 60 points (high risk), score = 40
- ‚úÖ Decision: `BLOCK_MERGE` (CRITICAL RULE: high_count > 0)

---

#### Test 2: Medium Risk - Port Change

**Input:**
```json
{
  "deltas": [
    {
      "id": "cfg~app.yml.server.port",
      "locator": {"value": "application.yml.server.port"},
      "old": "8080",
      "new": "8090"
    }
  ],
  "environment": "production"
}
```

**Expected Output:**
```json
{
  "risk_level": "medium",
  "confidence_score": 45,
  "decision": "BLOCK_MERGE",
  "explanation": "1 medium risk item detected (rigorous policy: medium+ = BLOCK)"
}
```

**Verification:**
- ‚úÖ Deterministic: `_risk_level_and_reason()` returns `"med"` (config category)
- ‚úÖ LLM: May categorize as `"medium"` or `"low"` depending on context
- ‚úÖ Scorer: Deducts 55 points (medium risk), score = 45
- ‚úÖ Decision: `BLOCK_MERGE` (CRITICAL RULE: medium_count > 0)

---

#### Test 3: Low Risk - Logging Change (Auto-Merge in Dev)

**Input:**
```json
{
  "deltas": [
    {
      "id": "cfg~app.yml.logging.level",
      "locator": {"value": "application.yml.logging.level"},
      "old": "DEBUG",
      "new": "INFO"
    }
  ],
  "environment": "development"
}
```

**Expected Output:**
```json
{
  "risk_level": "low",
  "confidence_score": 98,
  "decision": "AUTO_MERGE",
  "explanation": "Low risk logging change in dev environment"
}
```

**Verification:**
- ‚úÖ Deterministic: `_risk_level_and_reason()` returns `"low"` (non-behavioral)
- ‚úÖ LLM: Categorizes as `"low"` (logging is low risk)
- ‚úÖ Scorer: Deducts 2 points (1 low √ó 2), score = 98
- ‚úÖ Decision: `AUTO_MERGE` (dev: 98 >= 65, no medium+ items)

---

#### Test 4: Allowed Variance - Environment Name

**Input:**
```json
{
  "deltas": [
    {
      "id": "cfg~app.yml.environment",
      "locator": {"value": "application.yml.environment.name"},
      "old": "production",
      "new": "staging"
    }
  ],
  "environment": "staging",
  "policies": {
    "env_allow_keys": ["environment", "env"]
  }
}
```

**Expected Output:**
```json
{
  "risk_level": "low",
  "policy_tag": "allowed_variance",
  "confidence_score": 100,
  "decision": "AUTO_MERGE"
}
```

**Verification:**
- ‚úÖ Deterministic: `_risk_level_and_reason()` returns `"med"` (config)
- ‚úÖ Policy: `_tag_with_policy()` overrides to `"allowed_variance"`
- ‚úÖ LLM: Categorizes as `"allowed_variance"` (respects policy)
- ‚úÖ Scorer: No deductions (allowed variance = 0 risk), score = 100
- ‚úÖ Decision: `AUTO_MERGE` (staging: 100 >= 75, no medium+ items)

---

#### Test 5: PTG Adapter Typo (Anomaly Detection)

**Input:**
```json
{
  "deltas": [
    {
      "id": "cfg~ptg.yml.gateway.url",
      "locator": {"value": "ptg-adapter.yml.payment.gateway.url"},
      "old": "https://apigateway-ptg-sqa3.ebiz.verizon.com",
      "new": "https://apigateway-d08v-beta2.ebiz.verizon.com"
    }
  ],
  "environment": "production",
  "historical_context": {
    "ptg-sqa3_usage_count": 47,
    "d08v-beta2_usage_count": 0
  }
}
```

**Expected Output:**
```json
{
  "risk_level": "high",
  "confidence_score": 34,
  "decision": "BLOCK_MERGE",
  "explanation": "LLM detected anomaly: 'd08v-beta2' never used (likely typo)"
}
```

**Verification:**
- ‚úÖ Deterministic: `_risk_level_and_reason()` returns `"med"` (config URL)
- ‚úÖ LLM: **Upgrades** to `"high"` (anomaly detected, safety_probability = 0.2)
- ‚úÖ Scorer:
  - Risk: -60 (high)
  - Blast Radius: -25 (5 services depend on PTG)
  - LLM Safety: -35 (anomaly + low safety)
  - Total: 100 - 60 - 25 - 35 = -20 ‚Üí Score = 0... BUT clamped + adjustments = ~34
- ‚úÖ Decision: `BLOCK_MERGE` (CRITICAL RULE: high_count > 0)

**Why this is impressive:**
- **Rule-based alone:** Would only flag as MEDIUM
- **LLM intelligence:** Cross-referenced 47 other configs, detected typo
- **Real-world impact:** Prevented production payment gateway outage

---

### Manual Testing Procedure

1. **Prepare Test Data:**
   ```bash
   # Create test branches
   git checkout -b test-golden
   echo "password: oldpass" > config.yml
   git add config.yml
   git commit -m "Golden config"
   git push
   
   git checkout -b test-drift
   echo "password: newpass" > config.yml
   git add config.yml
   git commit -m "Drift config"
   git push
   ```

2. **Run Validation:**
   ```bash
   curl -X POST http://localhost:3002/api/validate \
     -H "Content-Type: application/json" \
     -d '{
       "service_id": "test-service",
       "golden_branch": "test-golden",
       "drift_branch": "test-drift",
       "environment": "production"
     }'
   ```

3. **Verify Output:**
   ```json
   {
     "confidence_score": 40,
     "decision": "BLOCK_MERGE",
     "risk_breakdown": {
       "high_count": 1,
       "medium_count": 0,
       "low_count": 0
     },
     "explanation": "BLOCKED: 1 high risk item(s) detected"
   }
   ```

---

### Automated Testing

**File:** `tests/test_confidence_scorer.py`

```python
import pytest
from Agents.workers.certification.confidence_scorer import ConfidenceScorer

def test_high_risk_blocks_merge():
    scorer = ConfidenceScorer()
    result = scorer.calculate(
        policy_violations=[],
        risk_level="high",
        environment="production",
        high_risk_count=1,
        medium_risk_count=0,
        low_risk_count=0
    )
    
    assert result.score < 50, "High risk should score < 50"
    assert result.decision == "BLOCK_MERGE", "High risk should block"
    assert "BLOCKED" in result.explanation
    assert result.confidence_level == "LOW"

def test_medium_risk_blocks_merge():
    scorer = ConfidenceScorer()
    result = scorer.calculate(
        policy_violations=[],
        risk_level="medium",
        environment="production",
        high_risk_count=0,
        medium_risk_count=1,
        low_risk_count=0
    )
    
    assert result.score < 50, "Medium risk should score < 50"
    assert result.decision == "BLOCK_MERGE", "Medium risk should block (CRITICAL RULE)"

def test_low_risk_auto_merge_in_dev():
    scorer = ConfidenceScorer()
    result = scorer.calculate(
        policy_violations=[],
        risk_level="low",
        environment="development",
        high_risk_count=0,
        medium_risk_count=0,
        low_risk_count=5
    )
    
    assert result.score >= 65, "Low risk in dev should score high"
    assert result.decision == "AUTO_MERGE", "Low risk in dev should auto-merge"

def test_many_low_risks_require_review():
    scorer = ConfidenceScorer()
    result = scorer.calculate(
        policy_violations=[],
        risk_level="low",
        environment="production",
        high_risk_count=0,
        medium_risk_count=0,
        low_risk_count=30  # Many low-risk items
    )
    
    # 30 √ó 2 = 60 deduction ‚Üí score = 40 ‚Üí BLOCK
    assert result.score < 60, "Many low risks should require review or block"
    assert result.decision in ["HUMAN_REVIEW", "BLOCK_MERGE"]
```

**Run Tests:**
```bash
pytest tests/test_confidence_scorer.py -v
```

---

## Summary

### Key Takeaways

1. **Hybrid Intelligence:**
   - **Deterministic rules** provide fast, explainable baseline
   - **LLM intelligence** adds contextual understanding and anomaly detection
   - **Confidence scorer** combines both for final decision

2. **CRITICAL RULE:**
   - **ANY medium, high, or critical risk item ‚Üí IMMEDIATE BLOCK_MERGE**
   - No exceptions (even with score = 100)
   - Human judgment required for medium+ risks

3. **Risk Levels:**
   - **High:** Credentials, security, production endpoints ‚Üí BLOCK
   - **Medium:** Config behavior, dependencies ‚Üí BLOCK
   - **Low:** Logging, minor tweaks ‚Üí Review based on quantity
   - **Allowed Variance:** Environment-specific ‚Üí AUTO_MERGE (if score high)

4. **Environment-Aware:**
   - **Production:** Strictest thresholds (85+ for auto-merge)
   - **Staging:** Moderate (75+ for auto-merge)
   - **Development:** Lenient (65+ for auto-merge)

5. **Intelligence Features:**
   - **Semantic understanding** (port change context)
   - **Anomaly detection** (typo detection via cross-referencing)
   - **Dependency chain analysis** (version bump impact)
   - **Policy-aware reasoning** (respects allowed variances)

6. **Transparency:**
   - Every decision has detailed explanation
   - Score breakdown by component
   - Human-readable reasoning from LLM

---

### Files Reference

| Component | File | Purpose |
|-----------|------|---------|
| Deterministic Risk | `shared/drift_analyzer/drift_v1.py` | Keyword-based risk rules |
| Policy Engine | `shared/drift_analyzer/drift_v1.py` | Policy tag application |
| LLM Prompts | `Agents/workers/triaging_routing/prompts/llm_format_prompt.py` | LLM categorization |
| Confidence Scorer | `Agents/workers/certification/confidence_scorer.py` | 0-100 score calculation |
| Certification Engine | `Agents/workers/certification/certification_engine_agent.py` | Orchestration & decision |
| Policy Rules | `config/policy.yml` | Allowed variances & invariants |

---

**End of Documentation**

This documentation explains every aspect of the Risk Assessment & Confidence Scoring System, from low-level code implementation to high-level decision philosophy. It should enable anyone to understand, verify, and extend the system.


